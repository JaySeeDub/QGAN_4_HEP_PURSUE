{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdd616-66d4-444b-aedb-b624a1169545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Imports import *\n",
    "from Plotting import *\n",
    "from Helper import *\n",
    "from Preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1953302-3ffa-4242-b1ab-34d30018bb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "jet_images_path = '../data/jet-images_Mass60-100_pT250-300_R1.25_Pix25.hdf5'\n",
    "jet_mass_data = HDF5File(jet_images_path, 'r')\n",
    "\n",
    "print(jet_mass_data.keys())\n",
    "print(jet_mass_data['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e699b9-4b51-4475-8451-81e1b31f2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=8)\n",
    "n_layers = 1\n",
    "\n",
    "# Random circuit parameters\n",
    "rand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 3))\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_feature_embedding(f, phi, interface=\"torch\", diff_method=\"backprop\", cachesize=1000000):\n",
    "    qml.AmplitudeEmbedding(features=f, wires=range(8), normalize=True, pad_with=0.)\n",
    "\n",
    "    # Random quantum circuit\n",
    "    RandomLayers(rand_params, wires=list(range(8)))\n",
    "\n",
    "    # Measurement producing classical output values\n",
    "    return qml.state()\n",
    "    # return [qml.expval(qml.PauliZ(j)) for j in range(4)]\n",
    "\n",
    "state = quantum_feature_embedding(f=[1/4, 1/4, 1/4, 1/4, 1/4, 1/4, 1/4, 1/4, 1/4], phi=rand_params)\n",
    "print(state.shape)\n",
    "\n",
    "def quantum_feature_embedding_batch(x_batch, phi, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Applies quantum_feature_embedding_single to a batch of inputs.\n",
    "\n",
    "    Args:\n",
    "        x_batch (torch.Tensor): (B, D) input batch.\n",
    "        phi (np.ndarray or torch.Tensor): parameters for RandomLayers.\n",
    "        device (str): Device to return output on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output of shape (B, 2 ** n_qubits)\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for x in x_batch:\n",
    "        result = quantum_feature_embedding(x, phi)\n",
    "        result_tensor = result.real.to(device)\n",
    "        outputs.append(result_tensor)\n",
    "\n",
    "    return torch.stack(outputs)\n",
    "\n",
    "sample = torch.randn(16, 9).to(\"cuda\")  # Example batch\n",
    "phi_tensor = torch.tensor(rand_params, dtype=torch.float32)\n",
    "\n",
    "output = quantum_feature_embedding_batch(sample, phi_tensor)\n",
    "print(output.shape)\n",
    "\n",
    "class quantum_feature_embedding_batch(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Parameter(torch.tensor(np.random.uniform(high=2 * np.pi, size=(n_layers, 3)), dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        phi = self.phi\n",
    "        outputs = []\n",
    "        for x in x_batch:\n",
    "            result = quantum_feature_embedding(x, phi)\n",
    "            result_tensor = result.real.to(device)\n",
    "            outputs.append(result_tensor)\n",
    "    \n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d5313-2431-49ae-b8d5-08dec4d06388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator with Quantum Layer\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=9):\n",
    "        super().__init__()\n",
    "\n",
    "        self.noise = GaussianNoise(sigma=0.2)\n",
    "        \n",
    "        self.feature_gen = quantum_feature_embedding_batch()\n",
    "\n",
    "        self.image_gen1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 1x1 → 2x2\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.image_gen2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 2x2 → 4x4\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.image_gen3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),     # 4x4 → 8x8\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.image_gen4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),     # 8x8 → 16x16\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_feat):\n",
    "        img = self.feature_gen(z_feat).float()\n",
    "        # print(img.shape)\n",
    "        img = img.view(-1, 256, 1, 1)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen1(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen2(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen3(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen4(img)\n",
    "        # print(img.shape)\n",
    "        img = soft_threshold(img, threshold=0.001, sharpness=1000.0)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "                # Flattened image: 16x16 = 256\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(1, 64, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            spectral_norm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Dynamically get input dimensions\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, 16, 16)\n",
    "            flat_dim = self.image_encoder(dummy).shape[1] + 64\n",
    "\n",
    "        # Combined classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "   \n",
    "    def forward(self, img, features):\n",
    "        img_encoded = self.image_encoder(img)\n",
    "        feat_encoded = self.feature_encoder(features)\n",
    "        combined = torch.cat((img_encoded, feat_encoded), dim=1)\n",
    "\n",
    "        # print(img_encoded.shape)\n",
    "        # print(feat_encoded.shape)\n",
    "        # print(combined.shape[-1])\n",
    "\n",
    "        prob = self.classifier(combined)\n",
    "\n",
    "        return prob  # Shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65215bff-2af0-4a33-82ff-c3891ab88f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128*3\n",
    "n_events = int(.1 * jet_mass_data['image'].shape[0])\n",
    "\n",
    "dataset = JetDataset(jet_mass_data, n_events)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "kdes = feature_distributions(dataset)\n",
    "\n",
    "print(\"Number of samples:\", len(dataset))\n",
    "print(\"Image shape:\", dataset.images.shape)\n",
    "print(\"Feature shape:\", dataset.features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e997108-e834-424e-a5be-367cc0354f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "lr = 10e-4\n",
    "n_epochs = 300\n",
    "num = 4\n",
    "\n",
    "generator = Generator(latent_dim).cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "# Image shape: (16, 16)\n",
    "H, W = (16, 16)\n",
    "center_x, center_y = (W - 1) / 2, (H - 1) / 2\n",
    "\n",
    "# Coordinate grid\n",
    "x_coords, y_coords = torch.meshgrid(\n",
    "    torch.arange(W, dtype=torch.float32),\n",
    "    torch.arange(H, dtype=torch.float32),\n",
    "    indexing='ij')\n",
    "\n",
    "# Distance from center\n",
    "dists = (torch.sqrt((x_coords - center_x) ** 2 + (y_coords - center_y) ** 2)).cuda()\n",
    "dists = dists.unsqueeze(0)  # [1, 16, 16]\n",
    "\n",
    "tracked_fake_dR_mean = []\n",
    "tracked_fake_dR_std = []\n",
    "tracked_fake_pixel_mean = []\n",
    "tracked_fake_pixel_std = []\n",
    "\n",
    "tracked_real_dR_mean = []\n",
    "tracked_real_dR_std = []\n",
    "tracked_real_pixel_mean = []\n",
    "tracked_real_pixel_std = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d2c2b-ca7e-40d2-84dc-6855d1f66a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a previous model\n",
    "# Replace with the desired filename\n",
    "load_path = \"models/Q_gan_model_20250712_224355.pt\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Restore model weights\n",
    "generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
    "\n",
    "# Optionally restore tracking data\n",
    "g_losses = checkpoint[\"g_losses\"]\n",
    "d_losses = checkpoint[\"d_losses\"]\n",
    "\n",
    "tracked_fake_dR_mean = checkpoint[\"tracked_fake_dR_mean\"]\n",
    "tracked_fake_dR_std = checkpoint[\"tracked_fake_dR_std\"]\n",
    "tracked_fake_pixel_mean = checkpoint[\"tracked_fake_pixel_mean\"]\n",
    "tracked_fake_pixel_std = checkpoint[\"tracked_fake_pixel_std\"]\n",
    "\n",
    "tracked_real_dR_mean = checkpoint[\"tracked_real_dR_mean\"]\n",
    "tracked_real_dR_std = checkpoint[\"tracked_real_dR_std\"]\n",
    "tracked_real_pixel_mean = checkpoint[\"tracked_real_pixel_mean\"]\n",
    "tracked_real_pixel_std = checkpoint[\"tracked_real_pixel_std\"]\n",
    "\n",
    "print(f\"Loaded model from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcc0ac-3516-434c-a43a-ba172710b516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (real_image, real_features, flipped_image, flipped_features) in enumerate(dataloader):\n",
    "        \n",
    "        # All real data are normalized in the dataloader\n",
    "        real_feat = real_features.cuda()\n",
    "        real_flipped_feat = flipped_features.cuda()\n",
    "        real_img = real_image.unsqueeze(1).cuda()\n",
    "        real_flipped_img = flipped_image.unsqueeze(1).cuda()\n",
    "\n",
    "        # print(f\"Real: {real_img.shape}\")\n",
    "\n",
    "        # Codings should be label, eta, pT, mass that get passed directly to the discriminator\n",
    "        # Other values are pure noise and get passed to the generator, then those outputs passed to the discriminator\n",
    "\n",
    "        # Discriminator training\n",
    "        if i % 3 == 0:\n",
    "            optimizer_D.zero_grad()\n",
    "            discriminator.train()\n",
    "            # Generate fake samples\n",
    "            # Should be very easy to modify which values are passed as codings\n",
    "            z_codings = torch.cat([torch.randint(0, 2, (batch_size, 1)), \n",
    "                                  sample_fit_noise(kdes, num_samples=batch_size)[:,:]],\n",
    "                                  dim=1).cuda()\n",
    "            # z_noise = torch.randn(batch_size, 5, ).cuda()\n",
    "            # z_codings = torch.cat([z_codings, z_noise], dim=1)\n",
    "\n",
    "            # Discriminator gets z_codings + generated_features\n",
    "            fake_img = generator(z_codings)\n",
    "            # print(f\"Fake: {fake_img.shape}\")\n",
    "\n",
    "            # Generate eta-flipped data\n",
    "            flipped_z_codings = z_codings.clone()\n",
    "            flipped_z_codings[:, 1] *= -1\n",
    "            \n",
    "            fake_flipped_img = generator(flipped_z_codings)\n",
    "\n",
    "            # Get predictions and labels\n",
    "            real_disc_codings = real_feat[:,:num]\n",
    "            real_flipped_disc_codings = real_flipped_feat[:,:num]\n",
    "            fake_disc_codings = z_codings[:,:num]\n",
    "            fake_flipped_disc_codings = flipped_z_codings[:,:num]\n",
    "\n",
    "            real_pred = discriminator(real_img, real_disc_codings)\n",
    "            real_flipped_pred = discriminator(real_flipped_img, real_flipped_disc_codings)\n",
    "            fake_pred = discriminator(fake_img, fake_disc_codings)\n",
    "            fake_flipped_pred = discriminator(fake_flipped_img, fake_flipped_disc_codings)\n",
    "\n",
    "            preds = (torch.cat([real_pred, real_flipped_pred, fake_pred, fake_flipped_pred], dim=0)).squeeze(1)\n",
    "\n",
    "\n",
    "            # real_labels = torch.empty_like(real_pred).uniform_(0.7, 1.2)\n",
    "            # fake_labels = torch.empty_like(fake_pred).uniform_(0.0, 0.3)\n",
    "            # labels = (torch.cat([real_labels, fake_labels], dim=0)).cuda()\n",
    "\n",
    "            ones = torch.ones(2*len(fake_pred))\n",
    "            zeros = torch.zeros(2*len(real_pred))\n",
    "            labels = (torch.cat([ones, zeros], dim=0)).cuda()\n",
    "\n",
    "            # Discriminator loss is just its ability to distinguish\n",
    "            d_loss = torch.nn.BCELoss()(preds, labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Generator Training\n",
    "        if i % 1 == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            # generator.train()\n",
    "            \n",
    "            # Generate fake data\n",
    "            # Should be very easy to modify which values are passed as codings\n",
    "            z_codings = torch.cat([torch.randint(0, 2, (batch_size, 1)), \n",
    "                                  sample_fit_noise(kdes, num_samples=batch_size)[:,:]],\n",
    "                                  dim=1).cuda()\n",
    "            # z_noise = torch.randn(batch_size, 5, ).cuda()\n",
    "            # z_codings = torch.cat([z_codings, z_noise], dim=1)\n",
    "\n",
    "            # Discriminator gets z_codings + generated_features\n",
    "            fake_img = generator(z_codings)\n",
    "\n",
    "            # Generate eta-flipped data\n",
    "            flipped_z_codings = z_codings.clone()\n",
    "            flipped_z_codings[:, 1] *= -1\n",
    "            \n",
    "            fake_flipped_img = generator(flipped_z_codings)\n",
    "\n",
    "            # Get predictions and labels\n",
    "            fake_disc_codings = z_codings[:,:num]\n",
    "            fake_flipped_disc_codings = flipped_z_codings[:,:num]\n",
    "            \n",
    "            # Fooled discriminator loss\n",
    "            d_out = discriminator(fake_img, fake_disc_codings)\n",
    "            d_out_flip = discriminator(fake_flipped_img, fake_flipped_disc_codings)\n",
    "            real_pred = discriminator(real_img, real_disc_codings)\n",
    "            real_flipped_pred = discriminator(real_flipped_img, real_flipped_disc_codings)\n",
    "            \n",
    "            target = torch.ones_like(d_out)\n",
    "            bce = nn.BCELoss()\n",
    "            validity_loss = bce(d_out, target) + bce(d_out_flip, target)\n",
    "            \n",
    "            # ----- Fake ΔR Calculation -----\n",
    "            # Original\n",
    "            # Weighted: pixel * distance / sum(pixel)\n",
    "            weights = fake_img.squeeze(1)\n",
    "\n",
    "            dR = (weights * dists)\n",
    "\n",
    "            fake_dR_mean = dR.mean(dim = (1,2))\n",
    "            fake_dR_std = dR.std(dim = (1,2))\n",
    "            \n",
    "            # Pixel stats\n",
    "            fake_pixel_mean = weights.mean(dim = (1,2))\n",
    "            fake_pixel_std = weights.std(dim = (1,2))\n",
    "            \n",
    "            # Flipped\n",
    "            # Weighted: pixel * distance / sum(pixel)\n",
    "            weights = fake_flipped_img.squeeze(1)\n",
    "            dR = (weights * dists)\n",
    "\n",
    "            flipped_dR_mean = dR.mean(dim = (1,2))\n",
    "            flipped_dR_std = dR.std(dim = (1,2))\n",
    "            \n",
    "            # Pixel stats\n",
    "            flipped_pixel_mean = weights.squeeze(1).mean(dim = (1,2))\n",
    "            flipped_pixel_std = weights.squeeze(1).std(dim = (1,2))\n",
    "\n",
    "            real_dR_mean = z_codings[:,5]\n",
    "            real_dR_std = z_codings[:,6]\n",
    "            real_pixel_mean = z_codings[:,7]\n",
    "            real_pixel_std = z_codings[:,8]\n",
    "\n",
    "\n",
    "            # Statistical MSE loss\n",
    "            # dR_mean_loss = (torch.nn.MSELoss()(fake_dR_mean, real_dR_mean) + torch.nn.MSELoss()(flipped_dR_mean, real_dR_mean))\n",
    "            # dR_std_loss = (torch.nn.MSELoss()(fake_dR_std, real_dR_std) + torch.nn.MSELoss()(flipped_dR_std, real_dR_std))\n",
    "            # pixel_mean_loss = (torch.nn.MSELoss()(fake_pixel_mean, real_pixel_mean) + torch.nn.MSELoss()(flipped_pixel_mean, real_pixel_mean))\n",
    "            # pixel_std_loss = (torch.nn.MSELoss()(fake_pixel_std, real_pixel_std) + torch.nn.MSELoss()(flipped_pixel_std, real_pixel_std))\n",
    "            \n",
    "            # stat_loss = dR_mean_loss + dR_std_loss + 3*pixel_mean_loss + 2*pixel_std_loss\n",
    "\n",
    "            # print(dR_mean_loss)\n",
    "            # print(dR_std_loss)\n",
    "            # print(3*pixel_mean_loss)\n",
    "            # print(2*pixel_std_loss)\n",
    "\n",
    "            # Statistical KL Divergence loss\n",
    "            kl_total = 0\n",
    "            kl_total += kde_kl_divergence_torch(real_dR_mean, fake_dR_mean) / .001\n",
    "            kl_total += kde_kl_divergence_torch(real_dR_std, fake_dR_std) / .03\n",
    "            kl_total += kde_kl_divergence_torch(real_pixel_mean, fake_pixel_mean) / .0001\n",
    "            kl_total += kde_kl_divergence_torch(real_pixel_std, fake_pixel_std) / .01\n",
    "\n",
    "            stat_loss = kl_total\n",
    "\n",
    "            # Number non-zero loss\n",
    "            fake_nnz = soft_count_nonzero(fake_img, threshold=3e-3, sharpness=10000.0)\n",
    "            real_nnz = soft_count_nonzero(real_img, threshold=3e-3, sharpness=10000.0)\n",
    "            nnz_loss = torch.nn.MSELoss()(fake_nnz, real_nnz)\n",
    "\n",
    "            \n",
    "            # Total generator loss is the average of the discriminator's predictions of the original and flipped data\n",
    "            # + the difference between input and output dR and pixel statistics\n",
    "\n",
    "            alpha = .05\n",
    "            beta = .00001\n",
    "            chi = .0001\n",
    "\n",
    "            g_loss = (alpha*validity_loss + beta*nnz_loss + chi*stat_loss)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if n_epochs - epoch <= 10:\n",
    "                # Track fake stats\n",
    "                tracked_fake_dR_mean.append(fake_dR_mean.detach().cpu())\n",
    "                tracked_fake_dR_std.append(fake_dR_std.detach().cpu())\n",
    "                tracked_fake_pixel_mean.append(fake_pixel_mean.detach().cpu())\n",
    "                tracked_fake_pixel_std.append(fake_pixel_std.detach().cpu())\n",
    "                \n",
    "                # Track real stats from z_codings\n",
    "                tracked_real_dR_mean.append(z_codings[:,5].detach().cpu())\n",
    "                tracked_real_dR_std.append(z_codings[:,6].detach().cpu())\n",
    "                tracked_real_pixel_mean.append(z_codings[:,7].detach().cpu())\n",
    "                tracked_real_pixel_std.append(z_codings[:,8].detach().cpu())\n",
    "\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_losses.append(d_loss.item())\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{n_epochs}] [D loss: {d_losses[epoch]:.4f}] [G loss: {g_losses[epoch]:.4f}] [Validity_loss: {alpha*validity_loss:.4f}] \\n [Stat_loss: {chi*stat_loss:.4f}] [NNZ_loss: {beta*nnz_loss:.4f}]\") \n",
    "\n",
    "    z_img = torch.randn(batch_size, 256, 1, 1).cuda()\n",
    "\n",
    "    # Should be very easy to modify which values are passed as codings\n",
    "    z_codings = torch.cat([torch.randint(0, 2, (batch_size, 1)), \n",
    "                          sample_fit_noise(kdes, num_samples=batch_size)],\n",
    "                          dim=1).cuda()\n",
    "    # z_noise = torch.randn(batch_size, 5, ).cuda()\n",
    "    # z_feat1 = torch.cat([z_codings, z_noise], dim=1)\n",
    "    z_feat = torch.cat([z_codings], dim=1)\n",
    "\n",
    "    fake_images = generator(z_feat)\n",
    "    fake_feat = z_codings\n",
    "    fake_images.detach().cpu()\n",
    "    fake_feat.detach().cpu()\n",
    "    # real_images = next(iter(dataloader))[0][:1000].cpu()\n",
    "\n",
    "    # output_image = fake_images[:16]  # Save 16 generated samples\n",
    "    # output_image = (output_image + 1) / 2.0  # Scale to [0, 1]\n",
    "    # grid = (torchvision.utils.make_grid(output_image, nrow=4, normalize=True)).cpu()\n",
    "    # np_img = grid.permute(1, 2, 0).numpy()\n",
    "    # plt.imsave(f'classical_Jet_image_epoch_{epoch}.png', np_img)\n",
    "    plot_generated_samples(generator, dataset, kdes, batch_size=16, latent_dim=256)\n",
    "\n",
    "plot_metrics(g_losses, d_losses)\n",
    "\n",
    "## Last 10 epochs stats\n",
    "# Flatten all batches\n",
    "fake_dR_mean_vals = torch.cat(tracked_fake_dR_mean).numpy() / batch_size\n",
    "fake_dR_std_vals = torch.cat(tracked_fake_dR_std).numpy() / batch_size\n",
    "fake_pixel_mean_vals = torch.cat(tracked_fake_pixel_mean).numpy() / batch_size\n",
    "fake_pixel_std_vals = torch.cat(tracked_fake_pixel_std).numpy() / batch_size\n",
    "\n",
    "real_dR_mean_vals = torch.cat(tracked_real_dR_mean).numpy() / batch_size\n",
    "real_dR_std_vals = torch.cat(tracked_real_dR_std).numpy() / batch_size\n",
    "real_pixel_mean_vals = torch.cat(tracked_real_pixel_mean).numpy() / batch_size\n",
    "real_pixel_std_vals = torch.cat(tracked_real_pixel_std).numpy() / batch_size\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))  # 4 stats\n",
    "\n",
    "stat_titles = ['ΔR Mean', 'ΔR Std', 'Pixel Mean', 'Pixel Std']\n",
    "real_stats = [real_dR_mean_vals, real_dR_std_vals, real_pixel_mean_vals, real_pixel_std_vals]\n",
    "fake_stats = [fake_dR_mean_vals, fake_dR_std_vals, fake_pixel_mean_vals, fake_pixel_std_vals]\n",
    "\n",
    "for row in range(4):\n",
    "    ax = axs[row]\n",
    "\n",
    "    real_vals = real_stats[row]\n",
    "    fake_vals = fake_stats[row]\n",
    "\n",
    "    # Compute limits\n",
    "    lower = min(np.percentile(real_vals, 1), np.percentile(fake_vals, 1))\n",
    "    upper = max(np.percentile(real_vals, 99), np.percentile(fake_vals, 99))\n",
    "\n",
    "    # Truncate values\n",
    "    real_vals_trunc = real_vals[(real_vals >= lower) & (real_vals <= upper)]\n",
    "    fake_vals_trunc = fake_vals[(fake_vals >= lower) & (fake_vals <= upper)]\n",
    "\n",
    "    # Plot\n",
    "    ax.hist(real_vals_trunc, bins=1000, alpha=0.6, label='Real',\n",
    "            edgecolor='black', density=True, histtype='stepfilled')\n",
    "    ax.hist(fake_vals_trunc, bins=1000, alpha=0.6, label='Fake',\n",
    "            edgecolor='black', density=True, histtype='stepfilled')\n",
    "\n",
    "    ax.set_xlim(lower, upper)\n",
    "    ax.set_title(f\"{stat_titles[row]}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Real vs Fake Distributions by Statistic\", fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "## Save Model\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime(\"m%d_%H%M)\n",
    "\n",
    "# Save model states and tracked data in a single file\n",
    "save_path = f\"models/Q_gan_model_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    \"generator_state_dict\": generator.state_dict(),\n",
    "    \"discriminator_state_dict\": discriminator.state_dict(),\n",
    "    \"g_losses\": g_losses,\n",
    "    \"d_losses\": d_losses,\n",
    "    \"tracked_fake_dR_mean\": tracked_fake_dR_mean,\n",
    "    \"tracked_fake_dR_std\": tracked_fake_dR_std,\n",
    "    \"tracked_fake_pixel_mean\": tracked_fake_pixel_mean,\n",
    "    \"tracked_fake_pixel_std\": tracked_fake_pixel_std,\n",
    "    \"tracked_real_dR_mean\": tracked_real_dR_mean,\n",
    "    \"tracked_real_dR_std\": tracked_real_dR_std,\n",
    "    \"tracked_real_pixel_mean\": tracked_real_pixel_mean,\n",
    "    \"tracked_real_pixel_std\": tracked_real_pixel_std\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model and statistics saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b671c-2699-4b38-87d1-f426bc1630cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime(\"m%d_%H%M\")\n",
    "\n",
    "# Save model states and tracked data in a single file\n",
    "save_path = f\"models/Q_gan_model_{timestamp}.pt\"\n",
    "torch.save({\n",
    "    \"generator_state_dict\": generator.state_dict(),\n",
    "    \"discriminator_state_dict\": discriminator.state_dict(),\n",
    "    \"g_losses\": g_losses,\n",
    "    \"d_losses\": d_losses,\n",
    "    \"tracked_fake_dR_mean\": tracked_fake_dR_mean,\n",
    "    \"tracked_fake_dR_std\": tracked_fake_dR_std,\n",
    "    \"tracked_fake_pixel_mean\": tracked_fake_pixel_mean,\n",
    "    \"tracked_fake_pixel_std\": tracked_fake_pixel_std,\n",
    "    \"tracked_real_dR_mean\": tracked_real_dR_mean,\n",
    "    \"tracked_real_dR_std\": tracked_real_dR_std,\n",
    "    \"tracked_real_pixel_mean\": tracked_real_pixel_mean,\n",
    "    \"tracked_real_pixel_std\": tracked_real_pixel_std\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model and statistics saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9108a-548c-4d98-8010-cdf30950f9d1",
   "metadata": {},
   "source": [
    "![image.png](attachment:7aee3b98-579c-4bcb-9682-eb417b7e3a7a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e30701-7229-40cd-bce5-66aa85ad859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 4, 16\n",
    "n_images = n_rows * n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows * 1.5))\n",
    "\n",
    "vmin = dataset.images[:n_images].min()\n",
    "vmax = dataset.images[:n_images].max()\n",
    "\n",
    "# Show images and keep the first imshow object for colorbar\n",
    "im = None\n",
    "for i in range(n_images):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    ax = axes[row, col]\n",
    "    im = ax.imshow(dataset.images[i], cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add a single colorbar on the right of the grid\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.01, 0.7])  # [left, bottom, width, height]\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e51483-896a-4aaf-bc97-59aa84d823fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generated_samples(generator, discriminator, dataset, kdes, batch_size=100000)\n",
    "\n",
    "# Optional args:\n",
    "#     generator,\n",
    "#     discriminator,\n",
    "#     dataset,\n",
    "#     kdes,\n",
    "#     batch_size=16,\n",
    "#     latent_dim=256,\n",
    "#     codings=None,\n",
    "#     plot_distributions=True,\n",
    "#     compare_discriminator=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74caa734-1335-4784-af25-52e402d56e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
