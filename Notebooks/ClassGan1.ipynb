{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fdd616-66d4-444b-aedb-b624a1169545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 17:33:57.924555: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 17:33:57.965260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-03 17:33:57.965294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-03 17:33:57.966232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-03 17:33:57.972761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-03 17:34:03.226720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.nn.utils import spectral_norm\n",
    "from scipy.linalg import sqrtm\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from h5py import File as HDF5File\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1953302-3ffa-4242-b1ab-34d30018bb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['X_jets', 'm0', 'pt', 'y']>\n",
      "(139306, 3, 125, 125)\n",
      "(139306, 1)\n",
      "(139306, 1)\n",
      "(139306, 1)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "images_path = '../data/QCDToGGQQ_IMGjet_combined.h5'\n",
    "quark_gluon_data = HDF5File(images_path, 'r')\n",
    "\n",
    "print(quark_gluon_data.keys())\n",
    "print(quark_gluon_data['X_jets'].shape)\n",
    "print(quark_gluon_data['m0'].shape)\n",
    "print(quark_gluon_data['pt'].shape)\n",
    "print(quark_gluon_data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0b2de5-0db3-4e81-8720-6c53c8611ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (3371260093.py, line 111)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 111\u001b[0;36m\u001b[0m\n\u001b[0;31m    min_pixels = pixels.amin(2,3)]\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "def plot_generated_samples(generator, kdes, batch_size=4, latent_dim=625):\n",
    "    generator.eval()  # Disable dropout/batchnorm updates\n",
    "\n",
    "    # Latent vectors and codings\n",
    "    z_img = torch.randn(batch_size, latent_dim, 1, 1).cuda()\n",
    "    z_codings = torch.cat([\n",
    "        torch.randint(0, 2, (batch_size, 1)), \n",
    "        sample_fit_noise(kdes, num_samples=batch_size)\n",
    "    ], dim=1).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_samples = generator(z_codings).cpu()  # Shape: (B, 3, 25, 25)\n",
    "\n",
    "    print(\"Generated sample shape:\", gen_samples.shape)\n",
    "    print(\"Sample feature coding:\", np.round(z_codings[0].cpu().numpy(), 4))\n",
    "\n",
    "    # Plot: 3 rows (channels) x batch_size columns\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=batch_size, figsize=(batch_size * 1.5, 4.5))\n",
    "    channel_titles = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for c in range(3):\n",
    "            ax = axes[c, b]\n",
    "            ax.imshow(gen_samples[b, c].numpy(), cmap='viridis', vmin=0, vmax=1)\n",
    "            ax.axis('off')\n",
    "            if b == 0:\n",
    "                ax.set_ylabel(channel_titles[c], fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Generated Jet Images\", fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    generator.train()  # Restore training mode\n",
    "    \n",
    "def plot_metrics(g_losses, d_losses):\n",
    "    epochs = range(1, len(g_losses) + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(epochs, g_losses, label='Generator Loss', color='blue')\n",
    "    plt.plot(epochs, d_losses, label='Discriminator Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Losses Over Time')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_nonzero_pixel_distributions(images):\n",
    "    \"\"\"\n",
    "    images: torch.Tensor of shape (N, 3, 25, 25)\n",
    "    \"\"\"\n",
    "    channel_names = ['Channel 1', 'Channel 2', 'Channel 3']\n",
    "    colors = ['red', 'green', 'blue']\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for c in range(3):\n",
    "        pixels = images[:, c, :, :].flatten()\n",
    "        nonzero_pixels = pixels[pixels != 0]\n",
    "        axes[c].hist(nonzero_pixels, bins=10000, color=colors[c], alpha=0.75, edgecolor='black')\n",
    "        axes[c].set_title(f'Non-zero Pixel Values — {channel_names[c]}')\n",
    "        axes[c].set_xlabel('Value')\n",
    "        axes[c].set_ylabel('Count')\n",
    "        axes[c].grid(True)\n",
    "    axes[0].set_xlim(left=-.5, right=10)\n",
    "    axes[1].set_xlim(left=-.5, right=1)\n",
    "    axes[2].set_xlim(left=-.5, right=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_outlier_pixel_distributions(images):\n",
    "    \"\"\"\n",
    "    images: torch.Tensor of shape (N, 3, 25, 25)\n",
    "    \"\"\"\n",
    "    channel_names = ['Channel 1 Outliers', 'Channel 2 Outliers', 'Channel 3 Outliers']\n",
    "    colors = ['red', 'green', 'blue']\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for c in range(3):\n",
    "        pixels = images[:, c, :, :].flatten()\n",
    "        nonzero_pixels = pixels[pixels != 0]\n",
    "        axes[c].hist(nonzero_pixels, bins=10000, color=colors[c], alpha=0.75, edgecolor='black')\n",
    "        axes[c].set_title(f'Non-zero Pixel Values — {channel_names[c]}')\n",
    "        axes[c].set_xlabel('Value')\n",
    "        axes[c].set_ylabel('Count')\n",
    "        axes[c].grid(True)\n",
    "\n",
    "    axes[0].set_xlim(left=100)\n",
    "    axes[1].set_xlim(left=20)\n",
    "    axes[2].set_xlim(left=1)\n",
    "    axes[0].set_ylim(top=100)\n",
    "    axes[1].set_ylim(top=100)\n",
    "    axes[2].set_ylim(top=100)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_min_pixel_distributions(images):\n",
    "    \"\"\"\n",
    "    images: torch.Tensor of shape (N, 3, 25, 25)\n",
    "    \"\"\"\n",
    "    channel_names = ['Channel 1 Min', 'Channel 2 Min', 'Channel 3 Min']\n",
    "    colors = ['red', 'green', 'blue']\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for c in range(3):\n",
    "        pixels = images[:, c, :, :].flatten()\n",
    "        min_pixels = pixels.amin(2,3)]\n",
    "        axes[c].hist(nonzero_pixels, bins=10000, color=colors[c], alpha=0.75, edgecolor='black')\n",
    "        axes[c].set_title(f'Non-zero Pixel Values — {channel_names[c]}')\n",
    "        axes[c].set_xlabel('Value')\n",
    "        axes[c].set_ylabel('Count')\n",
    "        axes[c].grid(True)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a115fa1-e893-457b-aeae-5366707b520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions for kde distributions, noise, loss, etc\n",
    "def feature_distributions(dataset, x_max=None, bw_method='scott'):\n",
    "    all_features = torch.stack([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "    kde_labels = [\n",
    "        \"y\", \"m0\", \"pt\",\n",
    "        \"dR1_mean\", \"dR2_mean\", \"dR3_mean\",\n",
    "        \"dR1_std\", \"dR2_std\", \"dR3_std\",\n",
    "        \"pix1_mean\", \"pix2_mean\", \"pix3_mean\",\n",
    "        \"pix1_std\", \"pix2_std\", \"pix3_std\"\n",
    "    ]\n",
    "\n",
    "    feature_labels = [\n",
    "        r\"$y$\", r\"$m_0$\", r\"$p_T$\",\n",
    "        r\"$\\langle \\Delta R_1 \\rangle$\", r\"$\\langle \\Delta R_2 \\rangle$\", r\"$\\langle \\Delta R_3 \\rangle$\",\n",
    "        r\"$\\sigma_{\\Delta R_1}$\", r\"$\\sigma_{\\Delta R_2}$\", r\"$\\sigma_{\\Delta R_3}$\",\n",
    "        r\"$\\langle \\mathrm{Pix}_1 \\rangle$\", r\"$\\langle \\mathrm{Pix}_2 \\rangle$\", r\"$\\langle \\mathrm{Pix}_3 \\rangle$\",\n",
    "        r\"$\\sigma_{\\mathrm{Pix}_1}$\", r\"$\\sigma_{\\mathrm{Pix}_2}$\", r\"$\\sigma_{\\mathrm{Pix}_3}$\"\n",
    "    ]\n",
    "    \n",
    "    single_features = [\n",
    "        (0, r\"$y$\"),\n",
    "        (1, r\"$m_0$\"),\n",
    "        (2, r\"$p_T$\")\n",
    "    ]\n",
    "\n",
    "    grouped_features = [\n",
    "        ([3,4,5], r\"$\\langle \\Delta R \\rangle$\"),\n",
    "        ([6,7,8], r\"$\\sigma_{\\Delta R}$\"),\n",
    "        ([9,10,11], r\"$\\langle \\mathrm{Pix} \\rangle$\"),\n",
    "        ([12,13,14], r\"$\\sigma_{\\mathrm{Pix}}$\")\n",
    "    ]\n",
    "\n",
    "    n_single = len(single_features)\n",
    "    n_grouped = len(grouped_features)\n",
    "    ncols = 3\n",
    "    nrows = (n_single + n_grouped + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Get x_max for plot i\n",
    "    def get_xmax(i, default=None):\n",
    "        if x_max is None:\n",
    "            return default\n",
    "        if isinstance(x_max, (list, tuple, np.ndarray)):\n",
    "            if i < len(x_max):\n",
    "                return x_max[i]\n",
    "            else:\n",
    "                return default\n",
    "        return x_max  # scalar\n",
    "\n",
    "    kde_fits = {}\n",
    "\n",
    "    # Plot single features\n",
    "    for idx, (feat_idx, label) in enumerate(single_features):\n",
    "        data = all_features[:, feat_idx].cpu().numpy()\n",
    "        mean_val = data.mean()\n",
    "        std_val = data.std()\n",
    "\n",
    "        axs[idx].hist(data, bins=100, alpha=0.4, color='skyblue', edgecolor='black', density=True)\n",
    "        kde = gaussian_kde(data, bw_method)\n",
    "        kde_fits[kde_labels[feat_idx]] = kde\n",
    "        \n",
    "        x_vals = np.linspace(data.min(), data.max(), 500)\n",
    "        axs[idx].plot(x_vals, kde(x_vals), label='KDE', color='green')\n",
    "\n",
    "        xmax = get_xmax(idx, default=data.max())\n",
    "        if xmax is not None:\n",
    "            axs[idx].set_xlim(right=xmax)\n",
    "        axs[idx].set_ylim(bottom=0)\n",
    "        \n",
    "        axs[idx].set_title(label, fontsize=14)\n",
    "        axs[idx].set_xlabel(label)\n",
    "        axs[idx].set_ylabel(\"Density\")\n",
    "        axs[idx].grid(True)\n",
    "        axs[idx].legend()\n",
    "\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    layer_labels = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "\n",
    "    # Plot grouped features with overlay\n",
    "    for j, (indices, base_label) in enumerate(grouped_features):\n",
    "        ax_idx = n_single + j\n",
    "        ax = axs[ax_idx]\n",
    "\n",
    "        max_val = None\n",
    "        for i, feat_idx in enumerate(indices):\n",
    "            data = all_features[:, feat_idx].cpu().numpy()\n",
    "            mean_val = data.mean()\n",
    "            std_val = data.std()\n",
    "\n",
    "            bins = 500 if feat_idx % 3 == 0 else 250\n",
    "\n",
    "            counts, bins, _ = ax.hist(data, bins=bins, alpha=0.3, color=colors[i], edgecolor='black', density=True, label=f'{layer_labels[i]}')\n",
    "            kde = gaussian_kde(data, bw_method)\n",
    "            kde_fits[kde_labels[feat_idx]] = kde\n",
    "            x_vals = np.linspace(bins[0], bins[-1], 500)\n",
    "            ax.plot(x_vals, kde(x_vals), color=colors[i], lw=2)\n",
    "\n",
    "            max_val = max(max_val or 0, data.max())\n",
    "\n",
    "        xmax = get_xmax(ax_idx, default=max_val)\n",
    "        if xmax is not None:\n",
    "            ax.set_xlim(right=xmax)\n",
    "\n",
    "        ax.set_title(base_label, fontsize=14)\n",
    "        ax.set_xlabel(base_label)\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    for k in range(n_single + n_grouped, len(axs)):\n",
    "        axs[k].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Feature Distributions with Overlaid Detector Layers\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    return kde_fits\n",
    "\n",
    "def sample_fit_noise(kde_fits, num_samples=1):\n",
    "    feature_labels = [\n",
    "        \"m0\", \"pt\",\n",
    "        \"dR1_mean\", \"dR2_mean\", \"dR3_mean\",\n",
    "        \"dR1_std\", \"dR2_std\", \"dR3_std\",\n",
    "        \"pix1_mean\", \"pix2_mean\", \"pix3_mean\",\n",
    "        \"pix1_std\", \"pix2_std\", \"pix1_std\"\n",
    "    ]\n",
    "\n",
    "    samples = []\n",
    "    for label in feature_labels:\n",
    "        kde = kde_fits[label]\n",
    "        sampled = kde.resample(num_samples).T.squeeze()\n",
    "        samples.append(sampled)\n",
    "\n",
    "    stacked = np.stack(samples, axis=1)  # shape (num_samples, 15)\n",
    "    return torch.tensor(stacked, dtype=torch.float32)\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma > 0:\n",
    "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            sampled_noise = torch.randn_like(x) * scale\n",
    "            return x + sampled_noise\n",
    "        return x\n",
    "\n",
    "def kde_kl_divergence_torch(real, fake, bandwidth=0.1, num_points=1000, eps=1e-8):\n",
    "    min_val = torch.min(real.min(), fake.min()).detach()\n",
    "    max_val = torch.max(real.max(), fake.max()).detach()\n",
    "    support = torch.linspace(min_val, max_val, num_points, device=real.device).view(1, -1)  # [1, num_points]\n",
    "\n",
    "    def kde(samples):\n",
    "        # [B, 1] - expand to [B, num_points] for distance to each x\n",
    "        samples = samples.view(-1, 1)\n",
    "        dists = (samples - support) ** 2  # [B, num_points]\n",
    "        kernels = torch.exp(-0.5 * dists / bandwidth**2)\n",
    "        pdf = kernels.sum(dim=0)  # sum over samples -> [num_points]\n",
    "        pdf /= (pdf.sum() + eps)  # normalize\n",
    "        return pdf + eps  # avoid log(0)\n",
    "\n",
    "    p = kde(real)\n",
    "    q = kde(fake)\n",
    "\n",
    "    kl = (p * (p.log() - q.log())).sum()\n",
    "    return kl\n",
    "\n",
    "def max_per_channel(max_val, x: torch.Tensor) -> torch.Tensor:\n",
    "    x_perm = x.permute(1, 0, 2, 3)\n",
    "    x_flat = x_perm.reshape(x_perm.size(0), -1)\n",
    "    x_flat = x_flat.detach().cpu().numpy()\n",
    "    # q3 = torch.quantile(x_flat, 0.99, dim=1)   # (C, 1)\n",
    "    q3 = np.quantile(x_flat, max_val, axis=1)   # (C, 1)\n",
    "    q3 = torch.from_numpy(q3)\n",
    "    \n",
    "    return q3.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
    "    \n",
    "def min_per_channel(min_val, x: torch.Tensor) -> torch.Tensor:\n",
    "    x_flat = x.reshape(x.size(0)*x.size(1), -1)\n",
    "    x_flat = x_flat.detach().cpu().numpy()\n",
    "    # q3 = torch.quantile(x_flat, 0.01, dim=1)   # (C, 1)\n",
    "    q3 = np.quantile(x_flat, min_val, axis=1)   # (C, 1)\n",
    "    q3 = torch.from_numpy(q3)\n",
    "\n",
    "    return q3.view(x.size(0), 3, 1, 1)\n",
    "\n",
    "def MaxReLU(x):\n",
    "    return torch.minimum(x, torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e545b6-f694-40ef-8a58-afb6d3b0bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataset(Dataset):\n",
    "    def __init__(self, data, n_events):\n",
    "\n",
    "        self.images = torch.tensor(data['X_jets'][:n_events])\n",
    "        self.images = self.images[:,:,11:112, 11:112]\n",
    "        self.images = F.avg_pool2d(self.images, kernel_size=4, stride=4)        \n",
    "        self.flipped_images = torch.flip(self.images,[2])\n",
    "\n",
    "        print(f\"Images Shape: {self.images.shape}\")\n",
    "\n",
    "        # Normalize each channel\n",
    "        Min = self.images.amin(dim=(2,3), keepdim = True)\n",
    "        Max = self.images.amax(dim=(2,3), keepdim = True)\n",
    "        # Min = min_per_channel(.9, self.images)\n",
    "        # Max = max_per_channel(.995, self.images)\n",
    "\n",
    "        print(Min.shape)\n",
    "        print(Max.shape)\n",
    "\n",
    "        self.images = (self.images - Min) / (Max - Min)\n",
    "        self.flipped_images = (self.flipped_images - Min) /(Max - Min)\n",
    "\n",
    "        self.images[self.images > 1] = 1\n",
    "        self.flipped_images[self.flipped_images > 1] = 1\n",
    "        # self.images[self.images < 0] = 0\n",
    "        # self.flipped_images[self.flipped_images < 0] = 0\n",
    "\n",
    "        # self.images = torch.clamp(self.images, min=Min, max=Max)\n",
    "        # self.flipped_images = torch.clamp(self.flipped_images, min=Min, max=Max)\n",
    "\n",
    "        # ----- ΔR Calculation -----\n",
    "        # Image shape: (25, 25)\n",
    "        N, C, H, W = self.images.shape\n",
    "        center_x, center_y = (W - 1) / 2, (H - 1) / 2\n",
    "\n",
    "        # Coordinate grid\n",
    "        x_coords, y_coords = torch.meshgrid(\n",
    "            torch.arange(W, dtype=torch.float32),\n",
    "            torch.arange(H, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        )\n",
    "\n",
    "        # Distance from center\n",
    "        dists = torch.sqrt((x_coords - center_x) ** 2 + (y_coords - center_y) ** 2)\n",
    "        dists = dists.unsqueeze(0).unsqueeze(0)  # [1, 1, 25, 25]\n",
    "        print(f\"Distance Shape: {dists.shape}\")\n",
    "        # Weighted: sum(pixel * distance) / sum(pixel)\n",
    "        weights = self.images\n",
    "        weight_norm = torch.amax(self.images, dim=(2,3), keepdim=True)\n",
    "        \n",
    "        print(f\"Weight Shape: {weights.shape}\")\n",
    "        dR = (weights * dists) / weight_norm\n",
    "\n",
    "        # Compute per-image (channel) ΔR stats: output [N, C]\n",
    "        dR = (self.images * dists) / weight_norm\n",
    "        dR_mean = dR.view(N, C, -1).mean(dim=2)\n",
    "        dR_std = dR.view(N, C, -1).std(dim=2)\n",
    "        \n",
    "        # Pixel stats: per image channel\n",
    "        pixel_mean = self.images.view(N, C, -1).mean(dim=2)\n",
    "        pixel_std = self.images.view(N, C, -1).std(dim=2)\n",
    "\n",
    "        print(f\"dR Mean: {dR_mean.shape}\")\n",
    "        print(f\"dR STD: {dR_std.shape}\")\n",
    "        print(f\"Pixel Mean: {pixel_mean.shape}\")\n",
    "        print(f\"Pixel STD: {pixel_std.shape}\")\n",
    "\n",
    "        print(\"pt:\", data['pt'][:n_events].shape)\n",
    "        print(\"m0:\", data['m0'][:n_events].shape)\n",
    "        print(\"y:\", data['y'][:n_events].shape)\n",
    "\n",
    "        self.features = torch.tensor(np.concatenate([\n",
    "            data['y'][:n_events],\n",
    "            data['m0'][:n_events],\n",
    "            data['pt'][:n_events],\n",
    "            dR_mean,\n",
    "            dR_std,\n",
    "            pixel_mean,\n",
    "            pixel_std\n",
    "        ], axis=1), dtype=torch.float32)\n",
    "\n",
    "        print(\"Feature shape per event:\", self.features.shape)\n",
    "\n",
    "        # Normalize pt and mass features here also\n",
    "        # Normalize jet_mass (index 1)\n",
    "        self.features[:, 1] = (self.features[:, 1]-self.features[:, 1].min()) / (self.features[:, 1].max()-self.features[:, 1].min())\n",
    "        # Normalize jet_pt (index 2)\n",
    "        self.features[:, 2] = (self.features[:, 2]-self.features[:, 2].min()) / (self.features[:, 2].max()-self.features[:, 2].min())\n",
    "        \n",
    "        print(\"ΔR min:\", dR.min().item())\n",
    "        print(\"ΔR max:\", dR.max().item())\n",
    "        \n",
    "        print(\"ΔR mean min:\", dR_mean.min().item())\n",
    "        print(\"ΔR mean max:\", dR_mean.max().item())\n",
    "        \n",
    "        print(\"ΔR std min:\", dR_std.min().item())\n",
    "        print(\"ΔR std max:\", dR_std.max().item())\n",
    "        \n",
    "        print(\"Weights (pixel intensity) min:\", weights.min().item())\n",
    "        print(\"Weights (pixel intensity) max:\", weights.max().item())\n",
    "        \n",
    "        print(\"Pixel mean min:\", pixel_mean.min().item())\n",
    "        print(\"Pixel mean max:\", pixel_mean.max().item())\n",
    "        \n",
    "        print(\"Pixel std min:\", pixel_std.min().item())\n",
    "        print(\"Pixel std max:\", pixel_std.max().item())\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        features = self.features[idx]\n",
    "        flipped_image = self.flipped_images[idx]\n",
    "\n",
    "        return image, features, flipped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d5313-2431-49ae-b8d5-08dec4d06388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=25*25):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.noise = GaussianNoise(sigma=0.1)\n",
    "\n",
    "        self.feature_gen = nn.Sequential(\n",
    "            nn.Linear(15, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 25*25),\n",
    "            nn.LayerNorm(25*25),\n",
    "            self.noise\n",
    "        )\n",
    "\n",
    "        self.image_gen1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(25*25, 256, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "        )\n",
    "        self.image_gen2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "        )\n",
    "        self.image_gen3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "        )\n",
    "        self.image_gen4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.image_gen5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.LayerNorm(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.image_gen6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=6, stride=1, padding=0),\n",
    "            nn.LayerNorm(3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, z_feat):\n",
    "        img = self.feature_gen(z_feat)\n",
    "        img = img.view(-1, 625, 1, 1)\n",
    "        img = self.image_gen1(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen2(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen3(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen4(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen5(img)\n",
    "        # print(img.shape)\n",
    "        img = self.image_gen6(img)\n",
    "        # print(img.shape)\n",
    "        img = MaxReLU(img)\n",
    "        \n",
    "        # threshold = 0.001\n",
    "        # img = torch.where(img < threshold, torch.tensor(0.0, device=img.device), img)\n",
    "\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Flattened image: 25*25 = 625\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(3, 64, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Dynamically get input dimensions\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 25, 25)\n",
    "            flat_dim = self.image_encoder(dummy).shape[1] + 64\n",
    "            \n",
    "        # Combined classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "   \n",
    "    def forward(self, img, features):\n",
    "        img_encoded = self.image_encoder(img)\n",
    "        feat_encoded = self.feature_encoder(features)\n",
    "        combined = torch.cat((img_encoded, feat_encoded), dim=1)\n",
    "\n",
    "        # print(img_encoded.shape)\n",
    "        # print(feat_encoded.shape)\n",
    "        # print(combined.shape[-1])\n",
    "\n",
    "        prob = self.classifier(combined)\n",
    "\n",
    "        return prob  # Shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b37d1-d6bd-49e4-be24-91bb4b98d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quark_gluon_data.keys())\n",
    "print(quark_gluon_data['X_jets'].shape)\n",
    "print(quark_gluon_data['m0'].shape)\n",
    "print(quark_gluon_data['pt'].shape)\n",
    "print(quark_gluon_data['y'].shape)\n",
    "\n",
    "plt.imshow(np.swapaxes(quark_gluon_data['X_jets'][2526][0:1], 0,2))\n",
    "plot_data = quark_gluon_data['X_jets'][:3000]\n",
    "plot_nonzero_pixel_distributions(plot_data)\n",
    "plot_outlier_pixel_distributions(plot_data)\n",
    "plot_min_pixel_distributions(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c436833-6e2f-4928-8d9c-181238636ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.swapaxes(dataset.images[500][1,:,:], 0,1), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65215bff-2af0-4a33-82ff-c3891ab88f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_events = int(.1 * quark_gluon_data['X_jets'].shape[0])\n",
    "\n",
    "dataset = JetDataset(quark_gluon_data, n_events)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "kdes = feature_distributions(dataset, bw_method=.2)\n",
    "\n",
    "# x_max=[1.1, 1, 1, 1, 1, 0.1, 0.1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcc0ac-3516-434c-a43a-ba172710b516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracked_fake_dR_mean = []\n",
    "tracked_fake_dR_std = []\n",
    "tracked_fake_pixel_mean = []\n",
    "tracked_fake_pixel_std = []\n",
    "\n",
    "tracked_real_dR_mean = []\n",
    "tracked_real_dR_std = []\n",
    "tracked_real_pixel_mean = []\n",
    "tracked_real_pixel_std = []\n",
    "\n",
    "latent_dim = 25*25\n",
    "lr = 0.00003\n",
    "n_epochs = 30\n",
    "\n",
    "generator = Generator(latent_dim).cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "# Image shape: (25, 25)\n",
    "H, W = (25,25)\n",
    "center_x, center_y = (W - 1) / 2, (H - 1) / 2\n",
    "\n",
    "# Coordinate grid\n",
    "x_coords, y_coords = torch.meshgrid(\n",
    "    torch.arange(W, dtype=torch.float32),\n",
    "    torch.arange(H, dtype=torch.float32),\n",
    "    indexing='ij')\n",
    "\n",
    "# Distance from center\n",
    "dists = (torch.sqrt((x_coords - center_x) ** 2 + (y_coords - center_y) ** 2)).cuda()\n",
    "dists = dists.unsqueeze(0).unsqueeze(0)  # [1, 1, 25, 25]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_image, real_features, flipped_image) in enumerate(dataloader):\n",
    "\n",
    "        # All real data are normalized in the dataloader\n",
    "        real_feat = real_features.cuda()\n",
    "        real_img = real_image.cuda()\n",
    "        real_flipped_img = flipped_image.cuda()\n",
    "\n",
    "        # Codings should be label, pT, mass that get passed directly to the discriminator\n",
    "        # Other values are pure noise and get passed to the generator, then those outputs passed to the discriminator\n",
    "\n",
    "        # Discriminator training\n",
    "        if i % 5 == 0:\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Generate fake samples\n",
    "            # Should be very easy to modify which values are passed as codings\n",
    "            z_codings = torch.cat([torch.randint(0, 2, (batch_size, 1)), \n",
    "                                  sample_fit_noise(kdes, num_samples=batch_size)],\n",
    "                                  dim=1).cuda()\n",
    "            # z_noise = torch.randn(batch_size, 5, ).cuda()\n",
    "            # z_codings = torch.cat([z_codings, z_noise], dim=1)\n",
    "\n",
    "            # Discriminator gets z_codings + generated_features\n",
    "            fake_img = generator(z_codings)\n",
    "            \n",
    "            fake_flipped_img = torch.flip(fake_img, dims=[-2])  # flip vertically along height (η)\n",
    "            # print(fake_flipped_feat.shape)\n",
    "\n",
    "            # Get predictions and labels\n",
    "            real_disc_codings = real_feat[:,:3]\n",
    "            fake_disc_codings = z_codings[:,:3]\n",
    "\n",
    "            real_pred = discriminator(real_img, real_disc_codings)\n",
    "            real_flipped_pred = discriminator(real_flipped_img, real_disc_codings)\n",
    "            fake_pred = discriminator(fake_img, fake_disc_codings)\n",
    "            fake_flipped_pred = discriminator(fake_flipped_img, fake_disc_codings)\n",
    "\n",
    "            preds = (torch.cat([real_pred, real_flipped_pred, fake_pred, fake_flipped_pred], dim=0)).squeeze(1)\n",
    "\n",
    "            zeros = torch.zeros(2*len(real_pred))\n",
    "            ones = torch.ones(2*len(fake_pred))\n",
    "            labels = (torch.cat([ones, zeros], dim=0)).cuda()\n",
    "\n",
    "            # Discriminator loss is just its ability to distinguish\n",
    "            d_loss = torch.nn.BCELoss()(preds, labels)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Generator Training\n",
    "        if i % 1 == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate fake data\n",
    "            # Should be very easy to modify which values are passed as codings\n",
    "            z_codings = torch.cat([torch.randint(0, 2, (batch_size, 1)), \n",
    "                                  sample_fit_noise(kdes, num_samples=batch_size)],\n",
    "                                  dim=1).cuda()\n",
    "            # z_noise = torch.randn(batch_size, 5, ).cuda()\n",
    "            # z_codings = torch.cat([z_codings, z_noise], dim=1)\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_img = generator(z_codings)\n",
    "            fake_flipped_img = torch.flip(fake_img, dims=[-2])  # flip vertically along height (η)\n",
    "            fake_disc_codings = z_codings[:,:3]\n",
    "\n",
    "            # Fooled discriminator loss\n",
    "            d_out = discriminator(fake_img, fake_disc_codings)\n",
    "            d_out_flip = discriminator(fake_flipped_img, fake_disc_codings)\n",
    "            target = torch.ones_like(d_out)\n",
    "            bce = nn.BCELoss()\n",
    "            validity_loss = bce(d_out, target) + bce(d_out_flip, target)\n",
    "\n",
    "            # ----- Fake ΔR Calculation -----\n",
    "            # Compute per-image (channel) ΔR stats: output [N, C]\n",
    "            weights = fake_img\n",
    "\n",
    "            # Compute per-image (channel) ΔR stats: output [N, C]\n",
    "            dR = (weights * dists)\n",
    "            fake_dR_mean = dR.view(batch_size, 3, -1).mean(dim=2)\n",
    "            fake_dR_std = dR.view(batch_size, 3, -1).std(dim=2)\n",
    "            \n",
    "            # Pixel stats: per image channel\n",
    "            fake_pixel_mean = weights.view(batch_size, 3, -1).mean(dim=2)\n",
    "            fake_pixel_std = weights.view(batch_size, 3, -1).std(dim=2)\n",
    "\n",
    "            real_dR_mean = z_codings[:,3:6]\n",
    "            real_dR_std = z_codings[:,6:9]\n",
    "            real_pixel_mean = z_codings[:,9:12]\n",
    "            real_pixel_std = z_codings[:,12:15]\n",
    "\n",
    "            # Statistical loss\n",
    "\n",
    "            kl_total = 0\n",
    "            for c in range(3):\n",
    "                kl_total += kde_kl_divergence_torch(real_dR_mean[:, c], fake_dR_mean[:, c])\n",
    "                kl_total += kde_kl_divergence_torch(real_dR_std[:, c], fake_dR_std[:, c])\n",
    "                kl_total += kde_kl_divergence_torch(real_pixel_mean[:, c], fake_pixel_mean[:, c])\n",
    "                kl_total += 100* kde_kl_divergence_torch(real_pixel_std[:, c], fake_pixel_std[:, c])\n",
    "\n",
    "            stat_loss = kl_total\n",
    "            \n",
    "            # dR_mean_loss = torch.nn.MSELoss()(fake_dR_mean, real_dR_mean) / batch_size\n",
    "            # dR_std_loss = torch.nn.MSELoss()(fake_dR_std, real_dR_std) / batch_size\n",
    "            # pixel_mean_loss = torch.nn.MSELoss()(fake_pixel_mean, real_pixel_mean) / batch_size\n",
    "            # pixel_std_loss = torch.nn.MSELoss()(fake_pixel_std, real_pixel_std) / batch_size\n",
    "\n",
    "            # stat_loss = dR_mean_loss + dR_std_loss + 3*pixel_mean_loss + 2*pixel_std_loss\n",
    "\n",
    "            # Number non-zero loss\n",
    "            # Count non-zero pixels per image\n",
    "            # print(fake_img.min())\n",
    "            # print(real_img.min())\n",
    "            \n",
    "            # fake_nnz = (fake_img > 3e-3).float().sum(dim=[1, 2, 3]) /   # Shape: [batch]\n",
    "            # real_nnz = (real_img > 3e-3).float().sum(dim=[1, 2, 3]) /   # Shape: [batch]\n",
    "        \n",
    "            # Compute MSE across batch\n",
    "            #NNZ_Loss = torch.nn.MSELoss()(fake_nnz, real_nnz)\n",
    "\n",
    "            # Total generator loss is the average of the discriminator's predictions of the original and flipped data\n",
    "            # + the difference between input and output dR and pixel statistics\n",
    "\n",
    "            alpha = .3\n",
    "            beta = 30\n",
    "            chi = 1\n",
    "\n",
    "            g_loss = (alpha*validity_loss + chi*stat_loss) # + beta*NNZ_Loss\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if n_epochs - epoch <= 10:\n",
    "                # Track fake stats\n",
    "                tracked_fake_dR_mean.append(fake_dR_mean.detach().cpu())\n",
    "                tracked_fake_dR_std.append(fake_dR_std.detach().cpu())\n",
    "                tracked_fake_pixel_mean.append(fake_pixel_mean.detach().cpu())\n",
    "                tracked_fake_pixel_std.append(fake_pixel_std.detach().cpu())\n",
    "                \n",
    "                # Track real stats from z_codings\n",
    "                tracked_real_dR_mean.append(z_codings[:,3:6].detach().cpu())\n",
    "                tracked_real_dR_std.append(z_codings[:,6:9].detach().cpu())\n",
    "                tracked_real_pixel_mean.append(z_codings[:,9:12].detach().cpu())\n",
    "                tracked_real_pixel_std.append(z_codings[:,12:15].detach().cpu())\n",
    "\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_losses.append(d_loss.item())\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{n_epochs}] [D loss: {d_losses[epoch]:.4f}] [G loss: {g_losses[epoch]:.4f}] [Validity_loss: {alpha*validity_loss:.4f}] [Stat_loss: {chi*stat_loss:.4f}]\") # [NNZ_loss: {beta*NNZ_Loss:.4f}] \n",
    "\n",
    "    z_img = torch.randn(batch_size, 25*25, 1, 1).cuda() \n",
    "\n",
    "    # Should be very easy to modify which values are passed as codings\n",
    "    z_codings = torch.cat([torch.randint(0, 2, (batch_size, 1)), \n",
    "                          sample_fit_noise(kdes, num_samples=batch_size)],\n",
    "                          dim=1).cuda()\n",
    "    # z_noise = torch.randn(batch_size, 5, ).cuda()\n",
    "    # z_feat1 = torch.cat([z_codings, z_noise], dim=1)\n",
    "    z_feat = torch.cat([z_codings], dim=1)\n",
    "\n",
    "    fake_images = generator(z_feat)\n",
    "    fake_feat = z_codings\n",
    "    fake_images.detach().cpu()\n",
    "    fake_feat.detach().cpu()\n",
    "    # real_images = next(iter(dataloader))[0][:1000].cpu()\n",
    "\n",
    "    # output_image = fake_images[:16]  # Save 16 generated samples\n",
    "    # output_image = (output_image + 1) / 2.0  # Scale to [0, 1]\n",
    "    # grid = (torchvision.utils.make_grid(output_image, nrow=4, normalize=True)).cpu()\n",
    "    # np_img = grid.permute(1, 2, 0).numpy()\n",
    "    # plt.imsave(f'classical_Jet_image_epoch_{epoch}.png', np_img)\n",
    "    plot_generated_samples(generator, kdes, batch_size=16, latent_dim=625)\n",
    "\n",
    "\n",
    "plot_metrics(g_losses, d_losses)\n",
    "\n",
    "# Flatten all batches\n",
    "fake_dR_mean_vals = torch.cat(tracked_fake_dR_mean).numpy() / batch_size\n",
    "fake_dR_std_vals = torch.cat(tracked_fake_dR_std).numpy() / batch_size\n",
    "fake_pixel_mean_vals = torch.cat(tracked_fake_pixel_mean).numpy() / batch_size\n",
    "fake_pixel_std_vals = torch.cat(tracked_fake_pixel_std).numpy() / batch_size\n",
    "\n",
    "real_dR_mean_vals = torch.cat(tracked_real_dR_mean).numpy() / batch_size\n",
    "real_dR_std_vals = torch.cat(tracked_real_dR_std).numpy() / batch_size\n",
    "real_pixel_mean_vals = torch.cat(tracked_real_pixel_mean).numpy() / batch_size\n",
    "real_pixel_std_vals = torch.cat(tracked_real_pixel_std).numpy() / batch_size\n",
    "\n",
    "layer_colors = {\n",
    "    'Real': ['blue', 'red', 'green'],\n",
    "    'Fake': ['cyan', 'salmon', 'lime']\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 12))  # 4 stats × 3 layers\n",
    "\n",
    "stat_titles = ['ΔR Mean', 'ΔR Std', 'Pixel Mean', 'Pixel Std']\n",
    "real_stats = [real_dR_mean_vals, real_dR_std_vals, real_pixel_mean_vals, real_pixel_std_vals]\n",
    "fake_stats = [fake_dR_mean_vals, fake_dR_std_vals, fake_pixel_mean_vals, fake_pixel_std_vals]\n",
    "\n",
    "for row in range(4):  # for each statistic\n",
    "    for col in range(3):  # for each layer\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        # Extract per-layer values (assumed shape: [N, 3])\n",
    "        real_vals = real_stats[row][:, col]\n",
    "        fake_vals = fake_stats[row][:, col]\n",
    "\n",
    "        ax.hist(real_vals, bins=200, alpha=0.6, label='Real', color=layer_colors['Real'][col], edgecolor='black')\n",
    "        ax.hist(fake_vals, bins=200, alpha=0.6, label='Fake', color=layer_colors['Fake'][col], edgecolor='black')\n",
    "        ax.set_title(f\"{stat_titles[row]} — Layer {col + 1}\")\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Real vs Fake Distributions by Layer and Statistic\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e30701-7229-40cd-bce5-66aa85ad859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 3, 16\n",
    "n_images = n_rows * n_cols\n",
    "\n",
    "vmin = quark_gluon_data['X_jets'][:n_images].min()\n",
    "vmax = quark_gluon_data['X_jets'][:n_images].max()\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 1.5, 4.5))\n",
    "channel_titles = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "\n",
    "for b in range(n_cols):\n",
    "    for c in range(3):\n",
    "        ax = axes[c, b]\n",
    "        ax.imshow(dataset.images[-b, c], cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        if b == 0:\n",
    "            ax.set_ylabel(channel_titles[c], fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Reduced Jet Images\", fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263654f-3f52-433f-a4db-cd9d5130e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generated_samples(generator, kdes, batch_size=4, latent_dim=625, codings = None):\n",
    "    generator.eval()  # Disable dropout/batchnorm updates\n",
    "\n",
    "    if codings == None:\n",
    "        # Latent vectors and codings\n",
    "        z_img = torch.randn(batch_size, latent_dim, 1, 1).cuda()\n",
    "        z_codings = torch.cat([\n",
    "            torch.randint(0, 2, (batch_size, 1)), \n",
    "            sample_fit_noise(kdes, num_samples=batch_size)\n",
    "        ], dim=1).cuda()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            gen_samples = generator(z_codings).cpu()  # Shape: (B, 3, 25, 25)\n",
    "    \n",
    "        print(\"Generated sample shape:\", gen_samples.shape)\n",
    "        print(\"Sample feature codings:\", np.round(z_codings[:].cpu().numpy(), 4))\n",
    "\n",
    "    else:\n",
    "        z_codings = torch.cat(torch.tensor(batch_size), torch.tensor(codings))\n",
    "        with torch.no_grad():\n",
    "            gen_samples = generator(z_codings).cpu()  # Shape: (B, 3, 25, 25)\n",
    "    \n",
    "        print(\"Generated sample shape:\", gen_samples.shape)\n",
    "        print(\"Sample feature codings:\", np.round(z_codings[:].cpu().numpy(), 4))\n",
    "\n",
    "    # Plot: 3 rows (channels) x batch_size columns\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=batch_size, figsize=(batch_size * 1.5, 4.5))\n",
    "    channel_titles = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for c in range(3):\n",
    "            ax = axes[c, b]\n",
    "            ax.imshow(gen_samples[b, c].numpy(), cmap='viridis')\n",
    "            ax.axis('off')\n",
    "            if b == 0:\n",
    "                ax.set_ylabel(channel_titles[c], fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Generated Jet Images\", fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    generator.train()  # Restore training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e51483-896a-4aaf-bc97-59aa84d823fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generated_samples(generator, kdes, batch_size=16, latent_dim=625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b94c1e-2691-4bd0-9f2c-a723c503379f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
